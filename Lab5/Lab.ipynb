{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "736adc14",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageFolder\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\__init__.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\_meta_registrations.py:25\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;129;43m@register_meta\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mroi_align\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mmeta_roi_align\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspatial_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maligned\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrois\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrois must have shape as Tensor[K, 5]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\_meta_registrations.py:18\u001b[0m, in \u001b[0;36mregister_meta.<locals>.wrapper\u001b[1;34m(fn)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(fn):\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextension\u001b[49m\u001b[38;5;241m.\u001b[39m_has_ops():\n\u001b[0;32m     19\u001b[0m         get_meta_lib()\u001b[38;5;241m.\u001b[39mimpl(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mtorchvision, op_name), overload_name), fn)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "folder = ImageFolder('./a5_data_new/train', transform=torchvision.transforms.ToTensor())\n",
    "loader = DataLoader(folder, batch_size=8, shuffle=True)\n",
    "\n",
    "# Xexamples, Yexamples = next(iter(loader))\n",
    "\n",
    "# for i in range(8):\n",
    "#     plt.subplot(2,4,i+1)  \n",
    "#     img = Xexamples[i].numpy().transpose(1, 2, 0)    \n",
    "#     plt.imshow(img, interpolation='none')\n",
    "#     plt.title('NV' if Yexamples[i] else 'MEL')\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c443f35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with batch_size=32, learning_rate=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 201/201 [00:09<00:00, 21.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = 0.5602, val acc = 0.7428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 134/201 [00:07<00:03, 17.35it/s]"
     ]
    }
   ],
   "source": [
    "## making the cnn\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if CUDA is available, otherwise use CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "## first we train the model here is the function for training it\n",
    "\n",
    "# from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torchvision\n",
    "import numpy\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "## lended from lectures evaluates score\n",
    "def predict_and_evaluate(model, data):\n",
    "\n",
    "    all_gold = []\n",
    "    all_pred = []\n",
    "\n",
    "    for Xbatch, Ybatch in data:\n",
    "        # move the data to the GPU\n",
    "        Xbatch = Xbatch.to(device)\n",
    "        Ybatch = Ybatch.to(device)\n",
    "        outputs = model(Xbatch)\n",
    "        predictions = outputs.argmax(dim=1)\n",
    "        # move the data back to CPU\n",
    "        # and convert to numpy\n",
    "        all_gold.extend(Ybatch.cpu().numpy())\n",
    "        all_pred.extend(predictions.cpu().numpy())\n",
    "\n",
    "\n",
    "\n",
    "    return accuracy_score(all_gold, all_pred) \n",
    "\n",
    "def train_classifier(model, train_data, val_data, hyperparams):\n",
    "\n",
    "    # Deals with model updates. Adam is more effective than SGD.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=hyperparams['lr'])    \n",
    "    \n",
    "    # Cross-entropy loss because we have 10 classes.\n",
    "    # Note that the softmax is \"baked into\" this loss, so we should not\n",
    "    # use a softmax at the end.\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "      \n",
    "    # Some statistics.\n",
    "    acc_history = []\n",
    "\n",
    "    for epoch in range(hyperparams['n_epochs']):\n",
    "\n",
    "        # Set the model in training mode, enabling dropout if we use that.\n",
    "        model.train()\n",
    "        \n",
    "        loss_sum = 0\n",
    "\n",
    "        # For each batch\n",
    "        for Xbatch, Ybatch in tqdm(train_data):\n",
    "\n",
    "            # move the data to the GPU\n",
    "            Xbatch = Xbatch.to(device)\n",
    "            Ybatch = Ybatch.to(device)\n",
    "\n",
    "            # Apply the model. We don't know at this point what the model is.\n",
    "            # The output should be of the shape (batch_size, 10).\n",
    "            outputs = model(Xbatch)\n",
    "\n",
    "            # Apply the cross-entropy loss.\n",
    "            loss = loss_func(outputs, Ybatch)\n",
    "\n",
    "            # Update the model.\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_sum += loss.item()\n",
    "\n",
    "        # Set the model in evaluation mode. Disables dropout if present.\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Compute the accuracy on the validation data.\n",
    "            val_acc = predict_and_evaluate(model, val_data)\n",
    "                \n",
    "        mean_loss = loss_sum / len(train_data)\n",
    "\n",
    "        acc_history.append(val_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: loss = {mean_loss:.4f}, val acc = {val_acc:.4f}')\n",
    "    \n",
    "    return acc_history\n",
    "\n",
    "def train_model(model, trainFolder, testFolder,  epochs=10, batch_size=32, learning_rate=0.001):\n",
    "  \n",
    "    train_loader = DataLoader(trainFolder, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(testFolder, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Define hyperparameters\n",
    "    hyperparams = {\n",
    "        'n_epochs': epochs,\n",
    "        'lr': learning_rate\n",
    "    }\n",
    "\n",
    "    # Train the model\n",
    "    acc_history = train_classifier(model, train_loader, val_loader, hyperparams)\n",
    "\n",
    "    return acc_history\n",
    "                \n",
    "## plot the accuracy history\n",
    "def plot_accuracy_history(acc_history):\n",
    "    train_acc, val_acc = zip(*acc_history)\n",
    "    plt.plot(train_acc, label='Train Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "## Import train and test sets from the images. The Y values are the names of the folders\n",
    "Xtrain = []\n",
    "Ytrain = []\n",
    "Xval = []\n",
    "Yval = []\n",
    "\n",
    "trainFolder = torchvision.datasets.ImageFolder('./a5_data_new/train', transform=torchvision.transforms.ToTensor())\n",
    "valFolder = torchvision.datasets.ImageFolder('./a5_data_new/val', transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "\n",
    "\n",
    "# linear_model = nn.Linear(in_features=n_input_features, out_features=1)\n",
    "# acc_history = train_model(linear_model, Xtrain, Ytrain, Xval, Yval, epochs=10, batch_size=32, learning_rate=0.001)\n",
    "\n",
    "\n",
    "# Define the PyTorch model\n",
    "cnn_model = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, kernel_size=5),  # Adjusted input channels to 3 for RGB images\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=5),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 29 * 29, 50),  # Adjusted input size based on image dimensions after convolutions\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 10),\n",
    ").to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Set hyperparameters for grid search\n",
    "grid_params = {\n",
    "    'batch_size': [ 32, 64],\n",
    "    'learning_rate': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Perform grid search manually\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "for batch_size in grid_params['batch_size']:\n",
    "    for learning_rate in grid_params['learning_rate']:\n",
    "        print(f\"Training with batch_size={batch_size}, learning_rate={learning_rate}\")\n",
    "        acc_history = train_model(cnn_model, trainFolder, valFolder, epochs=5 , batch_size=batch_size, learning_rate=learning_rate)\n",
    "        val_acc = acc_history[-1]  # Get the validation accuracy of the last epoch\n",
    "        if val_acc > best_score:\n",
    "            best_score = val_acc\n",
    "            best_params = {'batch_size': batch_size, 'learning_rate': learning_rate}\n",
    "\n",
    "print(\"Best parameters found: \", best_params)\n",
    "print(\"Best validation accuracy: \", best_score)\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_batch_size = best_params['batch_size']\n",
    "best_learning_rate = best_params['learning_rate']\n",
    "acc_history = train_model(cnn_model, Xtrain, Ytrain, Xval, Yval, epochs=30, batch_size=best_batch_size, learning_rate=best_learning_rate)\n",
    "\n",
    "# Plot the accuracy history\n",
    "plot_accuracy_history(acc_history)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
